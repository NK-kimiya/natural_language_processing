{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52353eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kinar\\.conda\\envs\\natural_langugage_processing\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertJapaneseTokenizer, BertForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49a96891",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#トークナイザとBERTモデルをロード、モデルをGPUに載せる\n",
    "model_name = 'cl-tohoku/bert-base-japanese-whole-word-masking'\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(model_name)\n",
    "#BERTを文章穴埋めに応用したネットワーク\n",
    "bert_mlm = BertForMaskedLM.from_pretrained(model_name)\n",
    "bert_mlm = bert_mlm.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4fe2ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['今日', 'は', '[MASK]', 'へ', '行く', '。']\n"
     ]
    }
   ],
   "source": [
    "#文章の一部を特殊トークン[MASK]に置き換える\n",
    "text = '今日は[MASK]へ行く。'\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f231735f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#文章を符号化し、GPUに配置する\n",
    "input_ids = tokenizer.encode(text,return_tensors='pt')\n",
    "input_ids = input_ids.cuda()\n",
    "\n",
    "#BERTに入力し、分類スコアを得る\n",
    "with torch.no_grad():\n",
    "    output = bert_mlm(input_ids=input_ids)\n",
    "    scores = output.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c385cfac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今日は東京へ行く。\n"
     ]
    }
   ],
   "source": [
    "#5-6\n",
    "#ID列で'[MASK]'の位置を調べる\n",
    "mask_position = input_ids[0].tolist().index(4)\n",
    "\n",
    "#スコアが最も良いトークンのIDを取り出し、トークンに変換する\n",
    "id_best = scores[0,mask_position].argmax(-1).item()\n",
    "token_best = tokenizer.convert_ids_to_tokens(id_best)\n",
    "token_best = token_best.replace('##','')\n",
    "\n",
    "#[MASK]を求めたトークンで置き換える\n",
    "text = text.replace('[MASK]',token_best)\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "016f7eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今日は東京へ行く。\n",
      "今日はハワイへ行く。\n",
      "今日は学校へ行く。\n",
      "今日はニューヨークへ行く。\n",
      "今日はどこへ行く。\n",
      "今日は空港へ行く。\n",
      "今日はアメリカへ行く。\n",
      "今日は病院へ行く。\n",
      "今日はそこへ行く。\n",
      "今日はロンドンへ行く。\n"
     ]
    }
   ],
   "source": [
    "#上位10位のトークンに置き換える\n",
    "def predict_mask_topk(text,tokenizer,bert_mlm,num_topk):\n",
    "    \"\"\"\n",
    "    文章中の[MASK]をスコアの上位のトークンに置き換える\n",
    "    上位何位まで使うかは、num_topkで指定\n",
    "    出力は穴埋めされた文章のリストと、置き換えられたトークンのスコアのリスト\n",
    "    \"\"\"\n",
    "    #文章を符号化し、BERTで分類スコアを得る\n",
    "    input_ids = tokenizer.encode(text,return_tensors='pt')\n",
    "    input_ids = input_ids.cuda()\n",
    "    with torch.no_grad():\n",
    "        output = bert_mlm(input_ids=input_ids)\n",
    "    scores = output.logits\n",
    "    \n",
    "    #スコアが上位のトークンとスコアを求める\n",
    "    mask_position = input_ids[0].tolist().index(4)\n",
    "    topk = scores[0,mask_position].topk(num_topk)\n",
    "    ids_topk = topk.indices#トークンのID\n",
    "    tokens_topk = tokenizer.convert_ids_to_tokens(ids_topk)\n",
    "    scores_topk = topk.values.cpu().numpy() #スコア\n",
    "    \n",
    "    #文章中の[MASK]を上で求めたトークンで置き換える\n",
    "    text_topk = []#穴埋めされたテキストを追加\n",
    "    for token in tokens_topk:\n",
    "        token = token.replace('##','')\n",
    "        text_topk.append(text.replace('[MASK]',token,1))\n",
    "    \n",
    "    return text_topk,scores_topk\n",
    "\n",
    "text = '今日は[MASK]へ行く。'\n",
    "text_topk, _ = predict_mask_topk(text,tokenizer,bert_mlm,10)\n",
    "print(*text_topk,sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dd5c979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'今日は、東京へ行く。'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def greedy_prediction(text,tokenizer,bert_mlm):\n",
    "    \"\"\"\n",
    "    [MASK]を含む文章を入力して、貧欲法で穴埋めを行った文章を出力する\n",
    "    \"\"\"\n",
    "    #前から順に[MASK]を一つづつ、スコアの最も高いトークンに置き換える\n",
    "    for _ in range(text.count('[MASK]')):\n",
    "        text  = predict_mask_topk(text,tokenizer,bert_mlm,1)[0][0]\n",
    "    return text\n",
    "\n",
    "text = '今日は[MASK][MASK]へ行く。'\n",
    "greedy_prediction(text,tokenizer,bert_mlm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d20d3550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'今日は社会的にも'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '今日は[MASK][MASK][MASK][MASK]'\n",
    "greedy_prediction(text,tokenizer,bert_mlm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aaac31fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今日はお台場へ行く。\n",
      "今日はお祭りへ行く。\n",
      "今日はゲームセンターへ行く。\n",
      "今日はお風呂へ行く。\n",
      "今日はゲームショップへ行く。\n",
      "今日は東京ディズニーランドへ行く。\n",
      "今日はお店へ行く。\n",
      "今日は同じ場所へ行く。\n",
      "今日はあの場所へ行く。\n",
      "今日は同じ学校へ行く。\n"
     ]
    }
   ],
   "source": [
    "def beam_search(text,tokenizer,bert_mlm,num_topk):\n",
    "    \"\"\"\n",
    "    ビームサーチで文章の穴埋めを行う\n",
    "    \"\"\"\n",
    "    num_mask = text.count('[MASK]')\n",
    "    text_topk = [text]\n",
    "    scores_topk = np.array([0])\n",
    "    for _ in range(num_mask):\n",
    "        #現在得られている、それぞれの文章に対して、最初の[MASK]をスコアが上位のトークンで穴埋めする\n",
    "        text_candidates = [] #それぞれの文章を穴埋めした結果を追加する\n",
    "        score_candidates = []#穴埋めに使ったトークンのスコアを追加する\n",
    "        for text_mask, score in zip(text_topk,scores_topk):\n",
    "            text_topk_inner,scores_topk_inner = predict_mask_topk(\n",
    "                text_mask,tokenizer,bert_mlm,num_topk\n",
    "            )\n",
    "            text_candidates.extend(text_topk_inner)\n",
    "            score_candidates.append(score + scores_topk_inner)\n",
    "            \n",
    "        #穴埋めにより生成された文章の中から合計スコアの高いものを選ぶ\n",
    "        score_candidates = np.hstack(score_candidates)\n",
    "        idx_list = score_candidates.argsort() [::-1] [:num_topk]\n",
    "        text_topk = [text_candidates[idx] for idx in idx_list]\n",
    "        scores_topk = score_candidates[idx_list]\n",
    "    \n",
    "    return text_topk\n",
    "\n",
    "text = '今日は[MASK][MASK]へ行く。'\n",
    "text_topk = beam_search(text,tokenizer,bert_mlm,10)\n",
    "print(*text_topk,sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2c1554",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4226cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
