{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52353eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kinar\\.conda\\envs\\natural_langugage_processing\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertJapaneseTokenizer, BertForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49a96891",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ğŸ‘‰v4.50ğŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã¨BERTãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã€ãƒ¢ãƒ‡ãƒ«ã‚’GPUã«è¼‰ã›ã‚‹\n",
    "model_name = 'cl-tohoku/bert-base-japanese-whole-word-masking'\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(model_name)\n",
    "#BERTã‚’æ–‡ç« ç©´åŸ‹ã‚ã«å¿œç”¨ã—ãŸãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯\n",
    "bert_mlm = BertForMaskedLM.from_pretrained(model_name)\n",
    "bert_mlm = bert_mlm.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4fe2ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ä»Šæ—¥', 'ã¯', '[MASK]', 'ã¸', 'è¡Œã', 'ã€‚']\n"
     ]
    }
   ],
   "source": [
    "#æ–‡ç« ã®ä¸€éƒ¨ã‚’ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³[MASK]ã«ç½®ãæ›ãˆã‚‹\n",
    "text = 'ä»Šæ—¥ã¯[MASK]ã¸è¡Œãã€‚'\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f231735f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#æ–‡ç« ã‚’ç¬¦å·åŒ–ã—ã€GPUã«é…ç½®ã™ã‚‹\n",
    "input_ids = tokenizer.encode(text,return_tensors='pt')\n",
    "input_ids = input_ids.cuda()\n",
    "\n",
    "#BERTã«å…¥åŠ›ã—ã€åˆ†é¡ã‚¹ã‚³ã‚¢ã‚’å¾—ã‚‹\n",
    "with torch.no_grad():\n",
    "    output = bert_mlm(input_ids=input_ids)\n",
    "    scores = output.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c385cfac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä»Šæ—¥ã¯æ±äº¬ã¸è¡Œãã€‚\n"
     ]
    }
   ],
   "source": [
    "#5-6\n",
    "#IDåˆ—ã§'[MASK]'ã®ä½ç½®ã‚’èª¿ã¹ã‚‹\n",
    "mask_position = input_ids[0].tolist().index(4)\n",
    "\n",
    "#ã‚¹ã‚³ã‚¢ãŒæœ€ã‚‚è‰¯ã„ãƒˆãƒ¼ã‚¯ãƒ³ã®IDã‚’å–ã‚Šå‡ºã—ã€ãƒˆãƒ¼ã‚¯ãƒ³ã«å¤‰æ›ã™ã‚‹\n",
    "id_best = scores[0,mask_position].argmax(-1).item()\n",
    "token_best = tokenizer.convert_ids_to_tokens(id_best)\n",
    "token_best = token_best.replace('##','')\n",
    "\n",
    "#[MASK]ã‚’æ±‚ã‚ãŸãƒˆãƒ¼ã‚¯ãƒ³ã§ç½®ãæ›ãˆã‚‹\n",
    "text = text.replace('[MASK]',token_best)\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "016f7eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä»Šæ—¥ã¯æ±äº¬ã¸è¡Œãã€‚\n",
      "ä»Šæ—¥ã¯ãƒãƒ¯ã‚¤ã¸è¡Œãã€‚\n",
      "ä»Šæ—¥ã¯å­¦æ ¡ã¸è¡Œãã€‚\n",
      "ä»Šæ—¥ã¯ãƒ‹ãƒ¥ãƒ¼ãƒ¨ãƒ¼ã‚¯ã¸è¡Œãã€‚\n",
      "ä»Šæ—¥ã¯ã©ã“ã¸è¡Œãã€‚\n",
      "ä»Šæ—¥ã¯ç©ºæ¸¯ã¸è¡Œãã€‚\n",
      "ä»Šæ—¥ã¯ã‚¢ãƒ¡ãƒªã‚«ã¸è¡Œãã€‚\n",
      "ä»Šæ—¥ã¯ç—…é™¢ã¸è¡Œãã€‚\n",
      "ä»Šæ—¥ã¯ãã“ã¸è¡Œãã€‚\n",
      "ä»Šæ—¥ã¯ãƒ­ãƒ³ãƒ‰ãƒ³ã¸è¡Œãã€‚\n"
     ]
    }
   ],
   "source": [
    "#ä¸Šä½10ä½ã®ãƒˆãƒ¼ã‚¯ãƒ³ã«ç½®ãæ›ãˆã‚‹\n",
    "def predict_mask_topk(text,tokenizer,bert_mlm,num_topk):\n",
    "    \"\"\"\n",
    "    æ–‡ç« ä¸­ã®[MASK]ã‚’ã‚¹ã‚³ã‚¢ã®ä¸Šä½ã®ãƒˆãƒ¼ã‚¯ãƒ³ã«ç½®ãæ›ãˆã‚‹\n",
    "    ä¸Šä½ä½•ä½ã¾ã§ä½¿ã†ã‹ã¯ã€num_topkã§æŒ‡å®š\n",
    "    å‡ºåŠ›ã¯ç©´åŸ‹ã‚ã•ã‚ŒãŸæ–‡ç« ã®ãƒªã‚¹ãƒˆã¨ã€ç½®ãæ›ãˆã‚‰ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã®ã‚¹ã‚³ã‚¢ã®ãƒªã‚¹ãƒˆ\n",
    "    \"\"\"\n",
    "    #æ–‡ç« ã‚’ç¬¦å·åŒ–ã—ã€BERTã§åˆ†é¡ã‚¹ã‚³ã‚¢ã‚’å¾—ã‚‹\n",
    "    input_ids = tokenizer.encode(text,return_tensors='pt')\n",
    "    input_ids = input_ids.cuda()\n",
    "    with torch.no_grad():\n",
    "        output = bert_mlm(input_ids=input_ids)\n",
    "    scores = output.logits\n",
    "    \n",
    "    #ã‚¹ã‚³ã‚¢ãŒä¸Šä½ã®ãƒˆãƒ¼ã‚¯ãƒ³ã¨ã‚¹ã‚³ã‚¢ã‚’æ±‚ã‚ã‚‹\n",
    "    mask_position = input_ids[0].tolist().index(4)\n",
    "    topk = scores[0,mask_position].topk(num_topk)\n",
    "    ids_topk = topk.indices#ãƒˆãƒ¼ã‚¯ãƒ³ã®ID\n",
    "    tokens_topk = tokenizer.convert_ids_to_tokens(ids_topk)\n",
    "    scores_topk = topk.values.cpu().numpy() #ã‚¹ã‚³ã‚¢\n",
    "    \n",
    "    #æ–‡ç« ä¸­ã®[MASK]ã‚’ä¸Šã§æ±‚ã‚ãŸãƒˆãƒ¼ã‚¯ãƒ³ã§ç½®ãæ›ãˆã‚‹\n",
    "    text_topk = []#ç©´åŸ‹ã‚ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’è¿½åŠ \n",
    "    for token in tokens_topk:\n",
    "        token = token.replace('##','')\n",
    "        text_topk.append(text.replace('[MASK]',token,1))\n",
    "    \n",
    "    return text_topk,scores_topk\n",
    "\n",
    "text = 'ä»Šæ—¥ã¯[MASK]ã¸è¡Œãã€‚'\n",
    "text_topk, _ = predict_mask_topk(text,tokenizer,bert_mlm,10)\n",
    "print(*text_topk,sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dd5c979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ä»Šæ—¥ã¯ã€æ±äº¬ã¸è¡Œãã€‚'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def greedy_prediction(text,tokenizer,bert_mlm):\n",
    "    \"\"\"\n",
    "    [MASK]ã‚’å«ã‚€æ–‡ç« ã‚’å…¥åŠ›ã—ã¦ã€è²§æ¬²æ³•ã§ç©´åŸ‹ã‚ã‚’è¡Œã£ãŸæ–‡ç« ã‚’å‡ºåŠ›ã™ã‚‹\n",
    "    \"\"\"\n",
    "    #å‰ã‹ã‚‰é †ã«[MASK]ã‚’ä¸€ã¤ã¥ã¤ã€ã‚¹ã‚³ã‚¢ã®æœ€ã‚‚é«˜ã„ãƒˆãƒ¼ã‚¯ãƒ³ã«ç½®ãæ›ãˆã‚‹\n",
    "    for _ in range(text.count('[MASK]')):\n",
    "        text  = predict_mask_topk(text,tokenizer,bert_mlm,1)[0][0]\n",
    "    return text\n",
    "\n",
    "text = 'ä»Šæ—¥ã¯[MASK][MASK]ã¸è¡Œãã€‚'\n",
    "greedy_prediction(text,tokenizer,bert_mlm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d20d3550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ä»Šæ—¥ã¯ç¤¾ä¼šçš„ã«ã‚‚'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'ä»Šæ—¥ã¯[MASK][MASK][MASK][MASK]'\n",
    "greedy_prediction(text,tokenizer,bert_mlm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aaac31fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä»Šæ—¥ã¯ãŠå°å ´ã¸è¡Œãã€‚\n",
      "ä»Šæ—¥ã¯ãŠç¥­ã‚Šã¸è¡Œãã€‚\n",
      "ä»Šæ—¥ã¯ã‚²ãƒ¼ãƒ ã‚»ãƒ³ã‚¿ãƒ¼ã¸è¡Œãã€‚\n",
      "ä»Šæ—¥ã¯ãŠé¢¨å‘‚ã¸è¡Œãã€‚\n",
      "ä»Šæ—¥ã¯ã‚²ãƒ¼ãƒ ã‚·ãƒ§ãƒƒãƒ—ã¸è¡Œãã€‚\n",
      "ä»Šæ—¥ã¯æ±äº¬ãƒ‡ã‚£ã‚ºãƒ‹ãƒ¼ãƒ©ãƒ³ãƒ‰ã¸è¡Œãã€‚\n",
      "ä»Šæ—¥ã¯ãŠåº—ã¸è¡Œãã€‚\n",
      "ä»Šæ—¥ã¯åŒã˜å ´æ‰€ã¸è¡Œãã€‚\n",
      "ä»Šæ—¥ã¯ã‚ã®å ´æ‰€ã¸è¡Œãã€‚\n",
      "ä»Šæ—¥ã¯åŒã˜å­¦æ ¡ã¸è¡Œãã€‚\n"
     ]
    }
   ],
   "source": [
    "def beam_search(text,tokenizer,bert_mlm,num_topk):\n",
    "    \"\"\"\n",
    "    ãƒ“ãƒ¼ãƒ ã‚µãƒ¼ãƒã§æ–‡ç« ã®ç©´åŸ‹ã‚ã‚’è¡Œã†\n",
    "    \"\"\"\n",
    "    num_mask = text.count('[MASK]')\n",
    "    text_topk = [text]\n",
    "    scores_topk = np.array([0])\n",
    "    for _ in range(num_mask):\n",
    "        #ç¾åœ¨å¾—ã‚‰ã‚Œã¦ã„ã‚‹ã€ãã‚Œãã‚Œã®æ–‡ç« ã«å¯¾ã—ã¦ã€æœ€åˆã®[MASK]ã‚’ã‚¹ã‚³ã‚¢ãŒä¸Šä½ã®ãƒˆãƒ¼ã‚¯ãƒ³ã§ç©´åŸ‹ã‚ã™ã‚‹\n",
    "        text_candidates = [] #ãã‚Œãã‚Œã®æ–‡ç« ã‚’ç©´åŸ‹ã‚ã—ãŸçµæœã‚’è¿½åŠ ã™ã‚‹\n",
    "        score_candidates = []#ç©´åŸ‹ã‚ã«ä½¿ã£ãŸãƒˆãƒ¼ã‚¯ãƒ³ã®ã‚¹ã‚³ã‚¢ã‚’è¿½åŠ ã™ã‚‹\n",
    "        for text_mask, score in zip(text_topk,scores_topk):\n",
    "            text_topk_inner,scores_topk_inner = predict_mask_topk(\n",
    "                text_mask,tokenizer,bert_mlm,num_topk\n",
    "            )\n",
    "            text_candidates.extend(text_topk_inner)\n",
    "            score_candidates.append(score + scores_topk_inner)\n",
    "            \n",
    "        #ç©´åŸ‹ã‚ã«ã‚ˆã‚Šç”Ÿæˆã•ã‚ŒãŸæ–‡ç« ã®ä¸­ã‹ã‚‰åˆè¨ˆã‚¹ã‚³ã‚¢ã®é«˜ã„ã‚‚ã®ã‚’é¸ã¶\n",
    "        score_candidates = np.hstack(score_candidates)\n",
    "        idx_list = score_candidates.argsort() [::-1] [:num_topk]\n",
    "        text_topk = [text_candidates[idx] for idx in idx_list]\n",
    "        scores_topk = score_candidates[idx_list]\n",
    "    \n",
    "    return text_topk\n",
    "\n",
    "text = 'ä»Šæ—¥ã¯[MASK][MASK]ã¸è¡Œãã€‚'\n",
    "text_topk = beam_search(text,tokenizer,bert_mlm,10)\n",
    "print(*text_topk,sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2c1554",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4226cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
