{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMU97c1lRdT661PyITGvTOt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NK-kimiya/natural_language_processing/blob/master/BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERTモデルまとめ（Self-Attention理解用）\n",
        "\n",
        "## ✅ BERTモデルとは\n",
        "- TransformerのEncoder部分だけを用いた自然言語モデル\n",
        "\n",
        "### 特徴\n",
        "1. 文の前後関係を同時に理解できる（双方向）"
      ],
      "metadata": {
        "id": "iv_IS8oaBAjZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ✅ モデルの全体構造\n",
        "\n",
        "**STEP1：入力トークン列**  \n",
        "- 例：\"私はりんごが好きです\"\n",
        "\n",
        "**STEP2：トークンID・埋め込み**  \n",
        "- 入力をトークンに分割して、それぞれをベクトル（例：768次元）に変換\n",
        "\n",
        "**STEP3：Transformer Encoder × N層**  \n",
        "- 各層で Self-Attention + FeedForward を実行\n",
        "\n",
        "BERT内部の FeedForward は、「各トークンのベクトル」に対して個別に処理\n",
        "\n",
        "Flattenせず、トークンごとのベクトル構造（行列）は維持しながら全結合に通す\n",
        "\n",
        "**STEP4：トークンごとの出力ベクトルを取得**"
      ],
      "metadata": {
        "id": "Ou-u86I2BEnG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ✅ Self-Attentionとは？\n",
        "- 各単語が、他の単語とどれくらい関係があるか（注目すべきか）を学ぶ層"
      ],
      "metadata": {
        "id": "taC_8woQBPAx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ✅ Self-Attentionの流れ（5トークン × 768次元で例）\n",
        "\n",
        "### ⑴ 入力行列\n",
        "X.shape = (5, 768) # 5トークン分の埋め込みベクトル\n",
        "<br>\n",
        "### ⑵ 重み行列による変換\n",
        "\n",
        "#### ① Query（問い）を作る重み行列\n",
        "- 「注目したい単語（何が気になるか）」を表す\n",
        "\n",
        "　　　W_Q: (768, 64) Q = X @ W_Q → (5, 64)\n",
        "　　　※(5トークン , 埋め込みの次元を64次元に圧縮)\n",
        "\n",
        "#### ② Key（鍵）を作る重み行列\n",
        "- 「自分がどう注目されるべきか（どんな特徴を持つか）」を表す\n",
        "\n",
        "　　　W_K: (768, 64) K = X @ W_K → (5, 64)\n",
        "　　　※(5トークン , 埋め込みの次元を64次元に圧縮)\n",
        "\n",
        "#### ③ Value（情報）を作る重み行列\n",
        "- 単語が実際に持っている意味的情報を表す\n",
        "\n",
        "　　　W_V: (768, 64) V = X @ W_V → (5, 64)\n",
        "　　　※(5トークン , 埋め込みの次元を64次元に圧縮)\n",
        "\n",
        "### ⑶ QとKの内積 → 注目スコアの計算\n",
        "- 内積によって各トークン間の関係性（類似度）を算出\n",
        "\n",
        "　　　AttentionScore = Q @ K.T → (5, 5)\n",
        "\n",
        "#### 例：Qの1トークン目とKの全トークンの内積（スカラー）を計算\n",
        "- Q₁・K₁ → スカラー①  \n",
        "- Q₁・K₂ → スカラー②  \n",
        "- Q₁・K₃ → スカラー③  \n",
        "- Q₁・K₄ → スカラー④  \n",
        "- Q₁・K₅ → スカラー⑤\n",
        "\n",
        "→ これをQ₂〜Q₅についても計算することでトークン同士の関係性が計算できる → 最終的に (5, 5) のスコア行列に\n",
        "\n",
        "---\n",
        "\n",
        "### ⑷ Softmaxでスコアを正規化（行方向に）\n",
        "\n",
        "AttentionWeight = softmax(AttentionScore / √64) → (5, 5)\n",
        "<br>\n",
        "### ⑸ 正規化されたスコアを用いて情報を合成\n",
        "Output = AttentionWeight @ V → (5, 64)\n",
        "\n",
        "→ 各トークンが文脈を反映した出力ベクトルを持つようになる\n",
        "\n",
        "### ⑹ BERTの出力を全結合層へ渡す\n",
        "Output（5, 64）→ Flatten or [CLS]抽出 → 全結合層\n",
        "※BERTでは入力の先頭に [CLS]（Classification）という特別なトークンが追加\n",
        "\n",
        "※文分類・感情分析・質問応答などのタスクでは、文全体の意味が必要\n",
        "\n",
        "※そこで、「[CLS]の出力ベクトル（例：768次元）」を使えば、文の代表ベクトルとして使える！\n",
        "## ✅ 学習で更新される重み\n",
        "\n",
        "1. Transformer内部の重み：  \n",
        "   - W_Q, W_K, W_V  \n",
        "   - 全結合層の重み  \n",
        "2. 出力層（タスクごとの分類器など）の全結合層の重み\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HY_rWqShBTWJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## メモ\n",
        "\n",
        "✔ 全結合層（Linear）は、1次元ベクトルを前提としているため、\n",
        "入力が行列や3次元配列（画像など）の場合、\n",
        "✅ Flattenしてベクトル化するか、ベクトル単位に分割して処理する必要がある。\n",
        "\n",
        "✔ 一方、Transformerは 行列を維持しながら計算しているように見えるが、\n",
        "実際は「各トークン（＝1ベクトル）ごと」に全結合を個別に適用しているだけなので、\n",
        "✅ 本質的には「ベクトルにLinearをかけている」に変わりはない。\n",
        "\n",
        "画像データもtransformerで学習できる。\n",
        "\n",
        "画像を小さな領域(パッチ)に分割\n",
        "\n",
        "例えば、224×224ピクセルの画像を16×16のパッチに分けると、196個のパッチができる。　各パッチは画像の (チャネル数, 高さ, 幅) を持つ3次元配列なので、\n",
        "それを 1次元ベクトルにFlattenすると、自然言語のトークンのように扱える\n"
      ],
      "metadata": {
        "id": "Ne3YCSTjI5Ih"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DuM6bwB2L4NV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}