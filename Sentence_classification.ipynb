{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3332a540",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kinar\\.conda\\envs\\natural_langugage_processing\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertJapaneseTokenizer,BertForSequenceClassification\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "#日本語の事前学習モデル\n",
    "MODEL_NAME = 'cl-tohoku/bert-base-japanese-whole-word-masking'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fc36d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)\n",
    "bert_sc = BertForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,num_labels=2\n",
    ")\n",
    "bert_sc = bert_sc.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ae9265d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# scoresのサイズ:\n",
      "torch.Size([3, 2])\n",
      "# predicted labels:\n",
      "tensor([1, 0, 1], device='cuda:0')\n",
      "# accuracy:\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "text_list = [\n",
    "    \"この映画は面白かった。\",\n",
    "    \"この映画の最後にはがっかりさせられた。\",\n",
    "    \"この映画を見て幸せな気持ちになった。\"\n",
    "]\n",
    "label_list = [1,0,1]\n",
    "\n",
    "#データの符号化\n",
    "encoding = tokenizer(\n",
    "    text_list,\n",
    "    padding='longest',\n",
    "    return_tensors='pt'\n",
    ")\n",
    "encoding = { k: v.cuda() for k, v in encoding.items()}\n",
    "labels = torch.tensor(label_list).cuda()\n",
    "\n",
    "#推論\n",
    "with torch.no_grad():\n",
    "    output = bert_sc.forward(**encoding)\n",
    "scores = output.logits#分類スコア\n",
    "labels_predicted = scores.argmax(-1)#スコアが最も値ラベル\n",
    "num_correct = (labels_predicted==labels).sum().item()#正解数\n",
    "accuracy = num_correct/labels.size(0)#精度\n",
    "\n",
    "print(\"# scoresのサイズ:\")\n",
    "print(scores.size())\n",
    "print(\"# predicted labels:\")\n",
    "print(labels_predicted)\n",
    "print(\"# accuracy:\")\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6c42e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6263, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#符号化\n",
    "encoding = tokenizer(\n",
    "    text_list,\n",
    "    padding='longest',\n",
    "    return_tensors='pt'\n",
    ")\n",
    "encoding['labels']=torch.tensor(label_list)#入力にラベルを加える\n",
    "encoding = {k: v.cuda() for k, v in encoding.items()}\n",
    "\n",
    "#ロスの計算\n",
    "output = bert_sc(**encoding)\n",
    "loss = output.loss#損失の取得\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8023c8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' は、内部コマンドまたは外部コマンド、\n",
      "操作可能なプログラムまたはバッチ ファイルとして認識されていません。\n",
      "tar: Error opening archive: Failed to open 'ldcc-20140209.tar.gz'\n"
     ]
    }
   ],
   "source": [
    "#データのダウンロード\n",
    "!wget https://www.rondhuit.com/download/ldcc-20140209.tar.gz\n",
    "\n",
    "#ファイルの解凍\n",
    "!tar -zxf ldcc-20140209.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe4f12dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# batch 0\n",
      "{'data': tensor([[0, 1],\n",
      "        [2, 3]]), 'labels': tensor([0, 1])}\n",
      "# batch 1\n",
      "{'data': tensor([[4, 5],\n",
      "        [6, 7]]), 'labels': tensor([2, 3])}\n"
     ]
    }
   ],
   "source": [
    "#データローダの作成\n",
    "dataset_for_loader = [\n",
    "    #data:2次元テンソルのデータ　labels：データのクラス\n",
    "    {'data':torch.tensor([0,1]),'labels':torch.tensor(0)},\n",
    "    {'data':torch.tensor([2,3]),'labels':torch.tensor(1)},\n",
    "    {'data':torch.tensor([4,5]),'labels':torch.tensor(2)},\n",
    "    {'data':torch.tensor([6,7]),'labels':torch.tensor(3)},\n",
    "]\n",
    "#リストで4つあるデータを2つのデータ(2バッチ)で分割\n",
    "loader = DataLoader(dataset_for_loader,batch_size=2)\n",
    "\n",
    "#データセットからミニバッチを取り出す\n",
    "for idx, batch in enumerate(loader):\n",
    "    print(f'# batch {idx}')\n",
    "    print(batch)\n",
    "    ##ファインチューニングではここでミニバッチごとの処理を行う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c91c498f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# batch 0\n",
      "{'data': tensor([[6, 7],\n",
      "        [4, 5]]), 'labels': tensor([3, 2])}\n",
      "# batch 1\n",
      "{'data': tensor([[2, 3],\n",
      "        [0, 1]]), 'labels': tensor([1, 0])}\n"
     ]
    }
   ],
   "source": [
    "#データをシャッフルしてミニバッチ単位に分割\n",
    "loader = DataLoader(dataset_for_loader,batch_size=2,shuffle=True)\n",
    "\n",
    "for idx, batch in enumerate(loader):\n",
    "    print(f'# batch {idx}')\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8e0039f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:44<00:00,  4.95s/it]\n"
     ]
    }
   ],
   "source": [
    "#全記事の文章データを取得して前処理\n",
    "\n",
    "#カテゴリーのリスト\n",
    "category_list = [\n",
    "    'dokujo-tsushin',\n",
    "    'it-life-hack',\n",
    "    'kaden-channel',\n",
    "    'livedoor-homme',\n",
    "    'movie-enter',\n",
    "    'peachy',\n",
    "    'smax',\n",
    "    'sports-watch',\n",
    "    'topic-news'\n",
    "]\n",
    "\n",
    "#トークナイザのロード\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "#各データの形式を整える\n",
    "max_length = 128\n",
    "dataset_for_loader = []\n",
    "for label,category in enumerate(tqdm(category_list)):\n",
    "    for file in glob.glob(f'./text/{category}/{category}*'):\n",
    "        lines = open(file,encoding='utf-8').read().splitlines()\n",
    "        text = '\\n' .join(lines[3:])#ファイルの4行目から抜き出す\n",
    "        encoding = tokenizer(\n",
    "            text,\n",
    "            max_length=max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True\n",
    "        )\n",
    "        encoding['labels'] = label #ラベルを追加\n",
    "        encoding = {k:torch.tensor(v) for k, v in encoding.items()}\n",
    "        dataset_for_loader.append(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f798e31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([    2,  2340, 19693, 10585, 28459,    35,  6692, 28493,    13,   501,\n",
      "           62,   101,    37,     8,   569,   335,     5,    51,     7,     9,\n",
      "         1040,     5,   616,     9,  2941,    18,  5602,   501,    20,    16,\n",
      "         4027, 10531,   140,    36,    73, 30020, 28457, 25127,    38,  1080,\n",
      "            5,    53,    28,   707,     5,    12,     9,    80,  3635,   205,\n",
      "           29,  2935,   604,  5846,  6503,    11,  4722,    16,   861,    13,\n",
      "            6, 12272, 24050,  2079,    11,    26,    62,    45,    28,  2451,\n",
      "           80,     8,    36, 24050,    14,    31,  1058,    75, 11218, 10531,\n",
      "         3676,   542,     5, 22130,     6,  5408,    16,  4831,    80,    29,\n",
      "           18,  7045,    26, 28456,  4799,   900,     6,   569,   335,     9,\n",
      "         1704,  1277,    15,  3318,  2575,    29,  2935,  5233,    75,    13,\n",
      "         3472,   459,    12,  8585,  3171,   312,  3676,   542, 22130,   241,\n",
      "            5,   709, 28696,  2180,    14, 12959,    71,     3]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1]), 'labels': tensor(0)}\n"
     ]
    }
   ],
   "source": [
    "print(dataset_for_loader[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fedb84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#データセットを分割\n",
    "random.shuffle(dataset_for_loader)#ランダムにシャッフル\n",
    "n = len(dataset_for_loader)\n",
    "n_train = int(0.6*n)\n",
    "n_val = int(0.2*n)\n",
    "dataset_train = dataset_for_loader[:n_train]#学習データ\n",
    "dataset_val = dataset_for_loader[n_train:n_train+n_val]#検証データ\n",
    "dataset_test = dataset_for_loader[n_train+n_val:]#テストデータ\n",
    "\n",
    "#データセットからデータローダを作成\n",
    "dataloader_train = DataLoader(\n",
    "    dataset_train,batch_size=32,shuffle=True\n",
    ")\n",
    "datasloader_val = DataLoader(dataset_val,batch_size=256)\n",
    "dataloader_test = DataLoader(dataset_test,batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0904feba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#文章分類モデルの定義\n",
    "from transformers import BertForSequenceClassification, BertConfig\n",
    "class BertSewuenceClassification_pl(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self,model_name,num_labels,lr):\n",
    "        '''\n",
    "        model_name:使用する事前学習済みモデル\n",
    "        num_labels:分類のクラス数\n",
    "        lr：学習率\n",
    "        '''\n",
    "        super().__init__()\n",
    "        #後からモデルのハイパーパラメータを確認\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        #BertForSequenceClassificationモデルの初期化\n",
    "        '''\n",
    "        model_name：使用するBERTの事前学習済みモデルの名前\n",
    "        num_class：分類タスクのクラス数\n",
    "        '''\n",
    "        config = BertConfig.from_pretrained(model_name, num_labels=num_labels)\n",
    "        self.bert_sc = BertForSequenceClassification.from_pretrained(model_name, config=config)\n",
    "        \n",
    "    def training_step(self,batch,batch_idx):\n",
    "        '''\n",
    "        batch：データローダーから取得される1バッチ分のデータ\n",
    "        batch_idx：現在のバッチ番号\n",
    "        '''\n",
    "        #バッチ内のモデルの出力\n",
    "        output = self.bert_sc(**batch)\n",
    "        #バッチ内の損失値\n",
    "        loss = output.loss\n",
    "        self.log('train_loss',loss)\n",
    "        return loss\n",
    "        \n",
    "    def validation_step(self,batch,batch_idx):\n",
    "        '''\n",
    "        batch：データローダーから取得される1バッチ分のデータ\n",
    "        batch_idx：現在のバッチ番号\n",
    "        '''\n",
    "        output = self.bert_sc(**batch)\n",
    "        val_loss = output.loss\n",
    "        self.log('val_loss',val_loss)\n",
    "        \n",
    "    def test_step(self,batch,batch_idx):\n",
    "        '''\n",
    "        batch：データローダーから取得される1バッチ分のデータ\n",
    "        batch_idx：現在のバッチ番号\n",
    "        '''\n",
    "        #テストデータのバッチ全体を表す辞書から正解ラベルを取り出す\n",
    "        labels = batch.pop('labels')\n",
    "        #事前に定義された分類タスク用のBERTモデルにbatchを渡し、出力を取得\n",
    "        output = self.bert_sc(**batch)\n",
    "        #最もスコアが高いクラスのインデックスを取得\n",
    "        labels_predicted = output.logits.argmax(-1)\n",
    "        #予測されたクラスと正解ラベルを比較し、一致するかどうかをブール値で取得\n",
    "        num_correct = (labels_predicted == labels).sum().item()\n",
    "        #精度を計算\n",
    "        accuracy = num_correct/labels.size(0)\n",
    "        self.log('accuracy',accuracy)\n",
    "        \n",
    "    #Adamで勾配を計算する\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(),lr=self.hparams.lr)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9254d557",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "#トレーニング中に特定の条件（モニタリングするメトリクス）を基にモデルを保存\n",
    "checkpoint = pl.callbacks.ModelCheckpoint(\n",
    "    #モニタリングするメトリクスとして、バリデーション損失を指定\n",
    "    monitor='val_loss',\n",
    "    #モニタリングするメトリクスの最小値を基準にモデルを保存\n",
    "    mode='min',\n",
    "    #最良のモデル（val_lossが最小のモデル）1つのみを保存\n",
    "    save_top_k=1,\n",
    "    #モデルの重み（パラメータ）のみを保存\n",
    "    save_weights_only=True,\n",
    "    #保存先ディレクトリを指定\n",
    "    dirpath='model/',\n",
    ")\n",
    "\n",
    "#トレーニング、バリデーション、テストのループを自動的に処理\n",
    "trainer = pl.Trainer(\n",
    "    #使用するGPUの数を指定\n",
    "    accelerator=\"gpu\",  # GPUを使用\n",
    "    devices=1,  \n",
    "    #トレーニングを実行するエポック（データセット全体の反復回数）の最大数\n",
    "    max_epochs=10,\n",
    "    #トレーニング中に実行するコールバック（追加処理）のリストを指定\n",
    "    callbacks=[checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ee7706f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Ti') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type                          | Params | Mode\n",
      "-----------------------------------------------------------------\n",
      "0 | bert_sc | BertForSequenceClassification | 110 M  | eval\n",
      "-----------------------------------------------------------------\n",
      "110 M     Trainable params\n",
      "0         Non-trainable params\n",
      "110 M     Total params\n",
      "442.497   Total estimated model params size (MB)\n",
      "0         Modules in train mode\n",
      "231       Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:00<00:00,  7.09it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kinar\\.conda\\envs\\natural_langugage_processing\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kinar\\.conda\\envs\\natural_langugage_processing\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 139/139 [00:38<00:00,  3.61it/s, v_num=0]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|█▋        | 1/6 [00:00<00:00, 90.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|███▎      | 2/6 [00:00<00:01,  2.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 3/6 [00:01<00:01,  2.25it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|██████▋   | 4/6 [00:01<00:00,  2.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|████████▎ | 5/6 [00:02<00:00,  1.88it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 6/6 [00:03<00:00,  1.80it/s]\u001b[A\n",
      "Epoch 1: 100%|██████████| 139/139 [00:38<00:00,  3.62it/s, v_num=0]   \u001b[A\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|█▋        | 1/6 [00:00<00:00, 111.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|███▎      | 2/6 [00:00<00:01,  2.98it/s] \u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 3/6 [00:01<00:01,  2.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|██████▋   | 4/6 [00:02<00:01,  1.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|████████▎ | 5/6 [00:02<00:00,  1.87it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 6/6 [00:03<00:00,  1.79it/s]\u001b[A\n",
      "Epoch 2: 100%|██████████| 139/139 [00:38<00:00,  3.61it/s, v_num=0]   \u001b[A\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|█▋        | 1/6 [00:00<00:00, 111.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|███▎      | 2/6 [00:00<00:01,  2.97it/s] \u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 3/6 [00:01<00:01,  2.24it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|██████▋   | 4/6 [00:02<00:01,  2.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|████████▎ | 5/6 [00:02<00:00,  1.87it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 6/6 [00:03<00:00,  1.80it/s]\u001b[A\n",
      "Epoch 3: 100%|██████████| 139/139 [00:38<00:00,  3.61it/s, v_num=0]   \u001b[A\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|█▋        | 1/6 [00:00<00:00, 111.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|███▎      | 2/6 [00:00<00:01,  2.97it/s] \u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 3/6 [00:01<00:01,  2.24it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|██████▋   | 4/6 [00:02<00:01,  1.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|████████▎ | 5/6 [00:02<00:00,  1.87it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 6/6 [00:03<00:00,  1.80it/s]\u001b[A\n",
      "Epoch 4: 100%|██████████| 139/139 [00:38<00:00,  3.59it/s, v_num=0]   \u001b[A\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|█▋        | 1/6 [00:00<00:00, 100.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|███▎      | 2/6 [00:00<00:01,  2.93it/s] \u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 3/6 [00:01<00:01,  2.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|██████▋   | 4/6 [00:02<00:01,  1.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|████████▎ | 5/6 [00:02<00:00,  1.83it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 6/6 [00:03<00:00,  1.76it/s]\u001b[A\n",
      "Epoch 5: 100%|██████████| 139/139 [00:38<00:00,  3.58it/s, v_num=0]   \u001b[A\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|█▋        | 1/6 [00:00<00:00, 100.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|███▎      | 2/6 [00:00<00:01,  2.96it/s] \u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 3/6 [00:01<00:01,  2.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|██████▋   | 4/6 [00:02<00:01,  1.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|████████▎ | 5/6 [00:02<00:00,  1.87it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 6/6 [00:03<00:00,  1.79it/s]\u001b[A\n",
      "Epoch 6: 100%|██████████| 139/139 [00:38<00:00,  3.60it/s, v_num=0]   \u001b[A\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|█▋        | 1/6 [00:00<00:00, 99.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|███▎      | 2/6 [00:00<00:01,  2.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 3/6 [00:01<00:01,  2.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|██████▋   | 4/6 [00:02<00:01,  1.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|████████▎ | 5/6 [00:02<00:00,  1.86it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 6/6 [00:03<00:00,  1.79it/s]\u001b[A\n",
      "Epoch 7: 100%|██████████| 139/139 [00:38<00:00,  3.59it/s, v_num=0]   \u001b[A\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|█▋        | 1/6 [00:00<00:00, 90.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|███▎      | 2/6 [00:00<00:01,  2.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 3/6 [00:01<00:01,  2.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|██████▋   | 4/6 [00:02<00:01,  1.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|████████▎ | 5/6 [00:02<00:00,  1.86it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 6/6 [00:03<00:00,  1.79it/s]\u001b[A\n",
      "Epoch 8: 100%|██████████| 139/139 [00:38<00:00,  3.59it/s, v_num=0]   \u001b[A\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|█▋        | 1/6 [00:00<00:00, 99.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|███▎      | 2/6 [00:00<00:01,  2.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 3/6 [00:01<00:01,  2.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|██████▋   | 4/6 [00:02<00:01,  1.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|████████▎ | 5/6 [00:02<00:00,  1.86it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 6/6 [00:03<00:00,  1.79it/s]\u001b[A\n",
      "Epoch 9: 100%|██████████| 139/139 [00:38<00:00,  3.59it/s, v_num=0]   \u001b[A\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|█▋        | 1/6 [00:00<00:00, 90.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|███▎      | 2/6 [00:00<00:01,  2.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 3/6 [00:01<00:01,  2.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|██████▋   | 4/6 [00:02<00:01,  1.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|████████▎ | 5/6 [00:02<00:00,  1.86it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 6/6 [00:03<00:00,  1.79it/s]\u001b[A\n",
      "Epoch 9: 100%|██████████| 139/139 [00:42<00:00,  3.27it/s, v_num=0]   \u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 139/139 [00:42<00:00,  3.27it/s, v_num=0]\n"
     ]
    }
   ],
   "source": [
    "#モデルの生成\n",
    "'''\n",
    "MODEL_NAME：使用する事前学習済みのBERTモデルの名前\n",
    "num_labels：分類クラスの数\n",
    "lr：学習率\n",
    "'''\n",
    "model = BertSewuenceClassification_pl(\n",
    "    MODEL_NAME,num_labels=9,lr=1e-5\n",
    ")\n",
    "\n",
    "#トレーニング開始\n",
    "trainer.fit(model,dataloader_train,datasloader_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "19fde6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ベストモデルのファイル: C:\\Users\\kinar\\Desktop\\natural_language_processing\\model\\epoch=4-step=695.ckpt\n",
      "ベストモデルの検証データに対する損失: tensor(0.3802, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "best_model_path = checkpoint.best_model_path#ベストモデルのファイル\n",
    "print('ベストモデルのファイル:',checkpoint.best_model_path)\n",
    "print('ベストモデルの検証データに対する損失:',checkpoint.best_model_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "51d603c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-a3656af500f4127b\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-a3656af500f4127b\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#学習時の学習データや検証データに対する損失の値の変化をグラフで表示\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4ed82a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kinar\\.conda\\envs\\natural_langugage_processing\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\checkpoint_connector.py:145: `.test(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `.test(ckpt_path='best')` to use the best model or `.test(ckpt_path='last')` to use the last model. If you pass a value, this warning will be silenced.\n",
      "Restoring states from the checkpoint path at C:\\Users\\kinar\\Desktop\\natural_language_processing\\model\\epoch=4-step=695.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at C:\\Users\\kinar\\Desktop\\natural_language_processing\\model\\epoch=4-step=695.ckpt\n",
      "C:\\Users\\kinar\\.conda\\envs\\natural_langugage_processing\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 6/6 [00:03<00:00,  1.57it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         accuracy          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8690637946128845     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        accuracy         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8690637946128845    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy：0.87\n"
     ]
    }
   ],
   "source": [
    "#テストデータで評価\n",
    "test = trainer.test(dataloaders=dataloader_test)\n",
    "print(f'Accuracy：{test[0][\"accuracy\"]:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c8f8b051",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#PyTorch Lightningモデルのロード\n",
    "model = BertSewuenceClassification_pl.load_from_checkpoint(\n",
    "    best_model_path\n",
    ")\n",
    "\n",
    "#Transformers対応モデルを./model_transformersに保存\n",
    "model.bert_sc.save_pretrained('./model_trainformers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "98fded25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ファインチューニングしたモデルを読み込む\n",
    "bert_sc = BertForSequenceClassification.from_pretrained(\n",
    "    './model_trainformers/'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95035267",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
