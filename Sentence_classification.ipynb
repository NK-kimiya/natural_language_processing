{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3332a540",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kinar\\.conda\\envs\\natural_langugage_processing\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertJapaneseTokenizer,BertForSequenceClassification\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "#日本語の事前学習モデル\n",
    "MODEL_NAME = 'cl-tohoku/bert-base-japanese-whole-word-masking'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fc36d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)\n",
    "bert_sc = BertForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,num_labels=2\n",
    ")\n",
    "bert_sc = bert_sc.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ae9265d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# scoresのサイズ:\n",
      "torch.Size([3, 2])\n",
      "# predicted labels:\n",
      "tensor([0, 0, 0], device='cuda:0')\n",
      "# accuracy:\n",
      "0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "text_list = [\n",
    "    \"この映画は面白かった。\",\n",
    "    \"この映画の最後にはがっかりさせられた。\",\n",
    "    \"この映画を見て幸せな気持ちになった。\"\n",
    "]\n",
    "label_list = [1,0,1]\n",
    "\n",
    "#データの符号化\n",
    "encoding = tokenizer(\n",
    "    text_list,\n",
    "    padding='longest',\n",
    "    return_tensors='pt'\n",
    ")\n",
    "encoding = { k: v.cuda() for k, v in encoding.items()}\n",
    "labels = torch.tensor(label_list).cuda()\n",
    "\n",
    "#推論\n",
    "with torch.no_grad():\n",
    "    output = bert_sc.forward(**encoding)\n",
    "scores = output.logits#分類スコア\n",
    "labels_predicted = scores.argmax(-1)#スコアが最も値ラベル\n",
    "num_correct = (labels_predicted==labels).sum().item()#正解数\n",
    "accuracy = num_correct/labels.size(0)#精度\n",
    "\n",
    "print(\"# scoresのサイズ:\")\n",
    "print(scores.size())\n",
    "print(\"# predicted labels:\")\n",
    "print(labels_predicted)\n",
    "print(\"# accuracy:\")\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6c42e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7363, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#符号化\n",
    "encoding = tokenizer(\n",
    "    text_list,\n",
    "    padding='longest',\n",
    "    return_tensors='pt'\n",
    ")\n",
    "encoding['labels']=torch.tensor(label_list)#入力にラベルを加える\n",
    "encoding = {k: v.cuda() for k, v in encoding.items()}\n",
    "\n",
    "#ロスの計算\n",
    "output = bert_sc(**encoding)\n",
    "loss = output.loss#損失の取得\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8023c8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' は、内部コマンドまたは外部コマンド、\n",
      "操作可能なプログラムまたはバッチ ファイルとして認識されていません。\n",
      "tar: Error opening archive: Failed to open 'ldcc-20140209.tar.gz'\n"
     ]
    }
   ],
   "source": [
    "#データのダウンロード\n",
    "!wget https://www.rondhuit.com/download/ldcc-20140209.tar.gz\n",
    "\n",
    "#ファイルの解凍\n",
    "!tar -zxf ldcc-20140209.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe4f12dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# batch 0\n",
      "{'data': tensor([[0, 1],\n",
      "        [2, 3]]), 'labels': tensor([0, 1])}\n",
      "# batch 1\n",
      "{'data': tensor([[4, 5],\n",
      "        [6, 7]]), 'labels': tensor([2, 3])}\n"
     ]
    }
   ],
   "source": [
    "#データローダの作成\n",
    "dataset_for_loader = [\n",
    "    #data:2次元テンソルのデータ　labels：データのクラス\n",
    "    {'data':torch.tensor([0,1]),'labels':torch.tensor(0)},\n",
    "    {'data':torch.tensor([2,3]),'labels':torch.tensor(1)},\n",
    "    {'data':torch.tensor([4,5]),'labels':torch.tensor(2)},\n",
    "    {'data':torch.tensor([6,7]),'labels':torch.tensor(3)},\n",
    "]\n",
    "#リストで4つあるデータを2つのデータ(2バッチ)で分割\n",
    "loader = DataLoader(dataset_for_loader,batch_size=2)\n",
    "\n",
    "#データセットからミニバッチを取り出す\n",
    "for idx, batch in enumerate(loader):\n",
    "    print(f'# batch {idx}')\n",
    "    print(batch)\n",
    "    ##ファインチューニングではここでミニバッチごとの処理を行う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c91c498f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# batch 0\n",
      "{'data': tensor([[6, 7],\n",
      "        [0, 1]]), 'labels': tensor([3, 0])}\n",
      "# batch 1\n",
      "{'data': tensor([[4, 5],\n",
      "        [2, 3]]), 'labels': tensor([2, 1])}\n"
     ]
    }
   ],
   "source": [
    "#データをシャッフルしてミニバッチ単位に分割\n",
    "loader = DataLoader(dataset_for_loader,batch_size=2,shuffle=True)\n",
    "\n",
    "for idx, batch in enumerate(loader):\n",
    "    print(f'# batch {idx}')\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b8e0039f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:13<00:00,  1.53s/it]\n"
     ]
    }
   ],
   "source": [
    "#全記事の文章データを取得して前処理\n",
    "\n",
    "#カテゴリーのリスト\n",
    "category_list = [\n",
    "    'dokujo-tsushin',\n",
    "    'it-life-hack',\n",
    "    'kaden-channel',\n",
    "    'livedoor-homme',\n",
    "    'movie-enter',\n",
    "    'peachy',\n",
    "    'smax',\n",
    "    'sports-watch',\n",
    "    'topic-news'\n",
    "]\n",
    "\n",
    "#トークナイザのロード\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "#各データの形式を整える\n",
    "max_length = 128\n",
    "dataset_for_loader = []\n",
    "for label,category in enumerate(tqdm(category_list)):\n",
    "    for file in glob.glob(f'./text/{category}/{category}*'):\n",
    "        lines = open(file,encoding='utf-8').read().splitlines()\n",
    "        text = '\\n' .join(lines[3:])#ファイルの4行目から抜き出す\n",
    "        encoding = tokenizer(\n",
    "            text,\n",
    "            max_length=max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True\n",
    "        )\n",
    "        encoding['labels'] = label #ラベルを追加\n",
    "        encoding = {k:torch.tensor(v) for k, v in encoding.items()}\n",
    "        dataset_for_loader.append(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f798e31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([    2,  2340, 19693, 10585, 28459,    35,  6692, 28493,    13,   501,\n",
      "           62,   101,    37,     8,   569,   335,     5,    51,     7,     9,\n",
      "         1040,     5,   616,     9,  2941,    18,  5602,   501,    20,    16,\n",
      "         4027, 10531,   140,    36,    73, 30020, 28457, 25127,    38,  1080,\n",
      "            5,    53,    28,   707,     5,    12,     9,    80,  3635,   205,\n",
      "           29,  2935,   604,  5846,  6503,    11,  4722,    16,   861,    13,\n",
      "            6, 12272, 24050,  2079,    11,    26,    62,    45,    28,  2451,\n",
      "           80,     8,    36, 24050,    14,    31,  1058,    75, 11218, 10531,\n",
      "         3676,   542,     5, 22130,     6,  5408,    16,  4831,    80,    29,\n",
      "           18,  7045,    26, 28456,  4799,   900,     6,   569,   335,     9,\n",
      "         1704,  1277,    15,  3318,  2575,    29,  2935,  5233,    75,    13,\n",
      "         3472,   459,    12,  8585,  3171,   312,  3676,   542, 22130,   241,\n",
      "            5,   709, 28696,  2180,    14, 12959,    71,     3]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1]), 'labels': tensor(0)}\n"
     ]
    }
   ],
   "source": [
    "print(dataset_for_loader[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fedb84f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
