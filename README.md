# BERTモデルまとめ（Self-Attention理解用）

---

## ✅ BERTモデルとは
- TransformerのEncoder部分だけを用いた自然言語モデル  
- **特徴**：文の前後関係を同時に理解できる（双方向）

---

## ✅ モデルの全体構造

### STEP1：入力トークン列  
例：「私はりんごが好きです」

### STEP2：トークンID・埋め込み  
- 入力をトークンに分割し、それぞれをベクトル（例：768次元）に変換

### STEP3：Transformer Encoder × N層  
- 各層で **Self-Attention** ＋ **FeedForward** を実行  
- BERT内部のFeedForwardは「各トークンのベクトル」に対して個別に処理  
- Flattenせず、**トークンごとのベクトル構造（行列）を維持**したまま全結合に通す

### STEP4：トークンごとの出力ベクトルを取得

---

## ✅ Self-Attentionとは？
- 各単語が、他の単語とどれくらい関係があるか（注目すべきか）を学ぶ層

---

## ✅ Self-Attentionの流れ（5トークン × 768次元の例）

### ⑴ 入力行列　

X.shape = (5, 768) # 5トークン分の埋め込みベクトル　


### ⑵ 重み行列による変換
- **Query（問い）**  
  「注目したい単語（何が気になるか）」を表す  
  `W_Q: (768, 64)` → `Q = X @ W_Q` → `(5, 64)`

- **Key（鍵）**  
  「どう注目されるべきか（特徴）」を表す  
  `W_K: (768, 64)` → `K = X @ W_K` → `(5, 64)`

- **Value（情報）**  
  単語が持っている意味的情報  
  `W_V: (768, 64)` → `V = X @ W_V` → `(5, 64)`

### ⑶ QとKの内積 → 注目スコアの計算　

AttentionScore = Q @ K.T # → (5, 5)　

- 各トークン間のスコアを計算（例：Q₁とK₁〜K₅）

### ⑷ Softmaxで正規化（行方向)　

AttentionWeight = softmax(AttentionScore / √64) # → (5, 5)　

### ⑸ 情報を合成（出力ベクトル作成）　

Output = AttentionWeight @ V # → (5, 64)　


### ⑹ BERTの出力を全結合層へ渡す
- `Output (5, 64)` → Flatten or `[CLS]` を抽出 → 全結合層  
- `[CLS]`トークンは、文の代表ベクトルとして使用される

---

## ✅ 学習で更新される重み
- Transformer内部の重み：`W_Q`, `W_K`, `W_V`  
- 全結合層の重み  
- 出力層の重み（タスクごとの分類器など）

---

## 🧠 メモ：全結合層とTransformerの違い

- **全結合層（Linear）**：  
  - 入力がベクトル前提  
  - 行列や画像などは Flattenしてから処理する

- **Transformer**：  
  - 一見、行列処理に見えるが、**各トークンごとにLinearをかけているだけ**

---

## ✅ Transformerは画像にも応用可能！

- 画像を小さなパッチ（例：16×16）に分割  
- 例：224×224画像 → 196個のパッチ  
- 各パッチをFlattenし、トークンのように扱うことで、Transformerで画像処理が可能

---

# 📚 ニューラルネットワークの重みの形状まとめ

## ✅ 全結合層（Fully Connected / Linear）
- 重み形状：`(入力次元, 出力次元)`
- 例：入力64次元 → 出力5次元 → `重み：(64, 5)`

---

## ✅ RNN / LSTM
- すべて **2次元の行列**
- 例（LSTM）：
  - 入力重み：`(入力次元, 隠れ状態次元)`
  - 隠れ状態重み：`(隠れ状態次元, 隠れ状態次元)`
  - ゲートごとに重み行列がある

---

## ✅ CNN（畳み込み層）
- 重み形状：`(出力チャネル, 入力チャネル, カーネル高さ, カーネル幅)`
- 例：RGB入力 → 出力16チャネル → `重み：(16, 3, 3, 3)`

---

## ✅ BERT / Transformer の Attention部

- 重みの種類：
  - Query（Q）, Key（K）, Value（V）

- 各重みの形状：  
  `（埋め込み次元, ヘッド次元）`

- 意味：
  - Q：注目したい視点（問い）
  - K：意味を表す鍵
  - V：取得したい情報

---


