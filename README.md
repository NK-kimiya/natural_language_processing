# BERTモデルまとめ（Self-Attention理解用）

---

## ✅ BERTモデルとは
- TransformerのEncoder部分だけを用いた自然言語モデル  
- **特徴**：文の前後関係を同時に理解できる（双方向）

---

## ✅ モデルの全体構造

### STEP1：入力トークン列  
例：「私はりんごが好きです」

### STEP2：トークンID・埋め込み  
- 入力をトークンに分割し、それぞれをベクトル（例：768次元）に変換

### STEP3：Transformer Encoder × N層  
- 各層で **Self-Attention** ＋ **FeedForward** を実行  
- BERT内部のFeedForwardは「各トークンのベクトル」に対して個別に処理  
- Flattenせず、**トークンごとのベクトル構造（行列）を維持**したまま全結合に通す

### STEP4：トークンごとの出力ベクトルを取得

---

## ✅ Self-Attentionとは？
- 各単語が、他の単語とどれくらい関係があるか（注目すべきか）を学ぶ層

---

## ✅ Self-Attentionの流れ（5トークン × 768次元の例）

### ⑴ 入力行列　

X.shape = (5, 768) # 5トークン分の埋め込みベクトル
