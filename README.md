# BERTモデルまとめ（Self-Attention理解用）

---

## ✅ BERTモデルとは
- TransformerのEncoder部分だけを用いた自然言語モデル  
- **特徴**：文の前後関係を同時に理解できる（双方向）

---

## ✅ モデルの全体構造

### STEP1：入力トークン列  
例：「私はりんごが好きです」

### STEP2：トークンID・埋め込み  
- 入力をトークンに分割し、それぞれをベクトル（例：768次元）に変換

### STEP3：Transformer Encoder × N層  
- 各層で **Self-Attention** ＋ **FeedForward** を実行  
- BERT内部のFeedForwardは「各トークンのベクトル」に対して個別に処理  
- Flattenせず、**トークンごとのベクトル構造（行列）を維持**したまま全結合に通す

### STEP4：トークンごとの出力ベクトルを取得

---

## ✅ Self-Attentionとは？
- 各単語が、他の単語とどれくらい関係があるか（注目すべきか）を学ぶ層

---

## ✅ Self-Attentionの流れ（5トークン × 768次元の例）

### ⑴ 入力行列　

X.shape = (5, 768) # 5トークン分の埋め込みベクトル　


### ⑵ 重み行列による変換
- **Query（問い）**  
  「注目したい単語（何が気になるか）」を表す  
  `W_Q: (768, 64)` → `Q = X @ W_Q` → `(5, 64)`

- **Key（鍵）**  
  「どう注目されるべきか（特徴）」を表す  
  `W_K: (768, 64)` → `K = X @ W_K` → `(5, 64)`

- **Value（情報）**  
  単語が持っている意味的情報  
  `W_V: (768, 64)` → `V = X @ W_V` → `(5, 64)`

### ⑶ QとKの内積 → 注目スコアの計算　

AttentionScore = Q @ K.T # → (5, 5)　

- 各トークン間のスコアを計算（例：Q₁とK₁〜K₅）

### ⑷ Softmaxで正規化（行方向)　

AttentionWeight = softmax(AttentionScore / √64) # → (5, 5)

